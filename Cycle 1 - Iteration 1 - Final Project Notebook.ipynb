{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d58ce08-8fc4-4c43-9033-a9f61206ac26",
   "metadata": {},
   "source": [
    "## Iteration 1 description "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03998420-c155-4629-aac6-a986e6c0444c",
   "metadata": {},
   "source": [
    "- Text vectorization technique: CountVectorizer\n",
    "- ML algorith: Random Forest\n",
    "- Classifier: RandomForestClassifier\n",
    "- Normalization: PorterStemmer\n",
    "- Databalancing applied: None \n",
    "- Dataframe size: 39784 rows Ã— 26666 columns\n",
    "- Test accuracy: 0.40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e23909-fc2e-453c-acc7-82df002ecf18",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758fda44-4274-464d-9e06-4829a86c9af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b60e59-4166-4fbd-a76c-e59a758d5404",
   "metadata": {},
   "source": [
    "## Import CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f9432-c376-4be7-b68e-e15bb1df6a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweet_emotions.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e778d41-e951-400a-95ee-825c9c452cec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b649a45f-fccb-4220-a139-30ba65de4766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd7acb-14bb-4b2e-a13c-9ff7c26b0423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the style of the plot\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(df['sentiment'], bins=10, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Sentiment Scores', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Distribution of Sentiment Scores', fontsize=16)\n",
    "\n",
    "# Add grid and adjust tick parameters\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45, fontsize=12)  # Rotate x-axis labels by 45 degrees\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838080d7-bcc3-440a-838a-66dc55371369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243652d-d1bb-4145-939f-6c2e1c8bf865",
   "metadata": {},
   "source": [
    "Author note: This data is imbalanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce6881-ec51-46a1-9d46-d741eba2de76",
   "metadata": {},
   "source": [
    "Author note: Guidance to reduce the number of emotion groups "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fbe4c-67d3-4098-94a7-3548d519b9ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "Author note: In order to guide the workload of a Customer Success Manager, the following grouping would make sense: \n",
    "\n",
    "Positive Sentiments:\n",
    "\n",
    "- happiness\n",
    "- love\n",
    "- relief\n",
    "- enthusiasm\n",
    "\n",
    "Neutral Sentiments:\n",
    "\n",
    "- neutral\n",
    "- surprise\n",
    "- fun\n",
    "\n",
    "Negative Sentiments:\n",
    "\n",
    "- worry\n",
    "- sadness\n",
    "- hate\n",
    "- empty\n",
    "- boredom\n",
    "- anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014729ff-e949-41cb-b9d8-99cae3d957ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to map sentiments to sub-groups\n",
    "def map_sentiment_to_subgroup(sentiment):\n",
    "    if sentiment in ['empty', 'sadness', 'worry', 'hate', 'boredom', 'anger']:\n",
    "        return 'Negative'\n",
    "    elif sentiment in ['neutral', 'surprise', 'fun']:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "# Apply the function to create a new column for sub-groups\n",
    "df['sentiment_subgroup'] = df['sentiment'].apply(map_sentiment_to_subgroup)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d952a7-9c31-4709-ae59-821a4e646511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sentiment_subgroup'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9893c36b-a7c3-46a6-918d-fb67a7ae663d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style of the plot\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(df['sentiment_subgroup'], bins=10, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Sentiment Scores', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Distribution of Sentiment Scores', fontsize=16)\n",
    "\n",
    "# Add grid and adjust tick parameters\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45, fontsize=12)  # Rotate x-axis labels by 45 degrees\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb090f7-8096-4a1f-9525-68350f71d65c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for null values \n",
    "\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03755c-620a-468f-bd0f-753e11dd0c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for duplicates \n",
    "\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "duplicate_rows.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a222e-e815-4cfd-9eb9-859b90fef3af",
   "metadata": {},
   "source": [
    "### Remove names from content column before tokenizing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6523ccfd-7e89-415f-9e72-8e6b75d9d90e",
   "metadata": {},
   "source": [
    "Author note: As we know that any word beginning with @ is a name, we can assume that these words will not be useful predictors of sentiment. As such, I have chosen to delete all @ words prior to tokenizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad0cda-4a8f-42d2-a526-83f69f07277b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove words starting with '@' using a lambda function\n",
    "df['content'] = df['content'].apply(lambda x: ' '.join([word for word in x.split() if not word.startswith('@')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41853749-d285-4314-80a0-f6bc1011e9be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f821ff-7409-4a85-b6ee-cc32423d5505",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de21d98-baba-43a3-ab85-9942b6e7838a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk # Natural Language Toolkit\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Ensure that you have the 'punkt' tokenizer models downloaded\n",
    "\n",
    "# Tokenize the text in the 'content' column\n",
    "df['tokens'] = df['content'].apply(word_tokenize)\n",
    "\n",
    "# Display the DataFrame with tokens\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9762d35-8558-4472-9fbd-f07f4429ff52",
   "metadata": {},
   "source": [
    "Author note: This code will tokenize each text instance in the 'content' column and store the tokens in a new column called 'tokens' in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a6bc4-8189-4784-a76d-44daac45cd11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0bdd0c-d4b6-4d9c-b951-ff9f00dd3647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e6ff6-635d-4159-a0bb-628b8d233a59",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing tokenized text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff16077-8cba-4085-abe4-58e4c397778c",
   "metadata": {},
   "source": [
    "- Lowercasing: Convert all words to lowercase to ensure consistency and prevent the model from treating words with different cases as different entities.\n",
    "\n",
    "- Removing punctuation: Remove punctuation marks such as commas, periods, and quotation marks. Punctuation generally does not carry semantic meaning and can introduce noise into the embeddings.\n",
    "\n",
    "- Removing stop words: As mentioned earlier, stop words are common words such as \"the,\" \"is,\" and \"and\" that occur frequently but typically do not contribute much to the meaning of the text. Removing them can reduce the dimensionality of the data and improve the efficiency of the Word2Vec model.\n",
    "\n",
    "- Handling numerical values: Depending on the specific use case, you may choose to remove or replace numerical values with placeholders. In some cases, numerical values may not be relevant to the semantics of the text and can be treated as noise.\n",
    "\n",
    "- Removing blank spaces: Should be treated as noise and deleted \n",
    "\n",
    "- Handling special characters: Special characters, symbols, and emojis may need to be handled appropriately based on the specific requirements of the application. You might choose to remove them, replace them with special tokens, or even treat them as separate entities\n",
    "\n",
    "- Removing blank rows\n",
    "\n",
    "- Removing words with just one letter: These are stealth stop words \n",
    "\n",
    "- Token normalization: This involves techniques such as stemming or lemmatization to reduce words to their base or root forms. For example, \"running,\" \"runs,\" and \"ran\" may all be reduced to the base form \"run.\" This helps in capturing semantic similarities between related words.\n",
    "\n",
    "- Handling out-of-vocabulary words: It's essential to handle words that are not present in the vocabulary of the Word2Vec model. This could involve techniques like using subword embeddings (e.g., FastText) or replacing unknown words with a special token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c883c10-19a4-4b2e-8f7e-617ce43cf8da",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078319e2-63f7-48bd-b378-ebd2d2866bf6",
   "metadata": {},
   "source": [
    "Convert all words to lowercase to ensure consistency and prevent the model from treating words with different cases as different entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfc8c4-157a-4497-876c-a9ffc39083a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to lowercase each word in a list of tokens\n",
    "def lowercase_tokens(tokens_list):\n",
    "    return [word.lower() for word in tokens_list]\n",
    "\n",
    "# Apply lowercase conversion to each list of tokens in the 'tokens' column\n",
    "df.loc[:, 'tokens'] = df['tokens'].apply(lowercase_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf91662-857a-4695-a23c-6b1ea666d29c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18642cc-0d67-40a7-a3b7-47ee849caf71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96f60a-25eb-4292-8007-a65017580135",
   "metadata": {},
   "source": [
    "Remove punctuation marks such as commas, periods, and quotation marks. Punctuation generally does not carry semantic meaning and can introduce noise into the embeddings. This code first checks if the value is a string before applying the regex substitution. If the value is not a string (e.g., NaN or float), it returns an empty string. This ensures that the re.sub function receives only string inputs, avoiding the TypeError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f988827-13d5-4ee8-aae7-59cb2fcdf9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Apply punctuation removal using a lambda function\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens_list: [re.sub(r'[^\\w\\s]', '', word) for word in tokens_list] if isinstance(tokens_list, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53750d1-da3b-42c9-a350-d743401b43ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e47f8-8987-4991-bc51-b04bdfb0e28f",
   "metadata": {},
   "source": [
    "### Removing stop-words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc833986-9bb5-4859-b8c8-082aa7b05e44",
   "metadata": {},
   "source": [
    "Stop words are common words such as \"the,\" \"is,\" and \"and\" that occur frequently but typically do not contribute much to the meaning of the text. Removing them can reduce the dimensionality of the data and improve the efficiency of the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423bad9e-fe03-4281-a565-03861e8f2d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the English stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from the 'tokens' column\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb686e-40f3-4d78-b36d-ebb0039241b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97fe52b-115b-4a99-8c30-84c74c93d6e9",
   "metadata": {},
   "source": [
    "### Removing blank spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf185e-fabc-4641-86c1-d19b1efd296c",
   "metadata": {},
   "source": [
    "Author note: Based on the print out of the tokens column above, I can see some blank spaces (' '). I will remove these to reduce noise in my model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ebc4e-d348-4ead-b915-c6ef9b246528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove blank spaces from the list of tokens\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens_list: [token for token in tokens_list if token.strip() != ''])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5cbb5f-8719-41e6-9e1f-0ebd822bfb52",
   "metadata": {},
   "source": [
    "### Handling numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29f3d7-2c20-4a1f-a96f-057360757c78",
   "metadata": {},
   "source": [
    "This code will remove all tokens containing only numerical values from the 'tokens' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e67e47-8d6b-4ff9-bce9-886984a15faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(lambda x: [token for token in x if not token.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7936d065-5fd4-4e5f-a6c6-5e2e9e302f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa630009-8d09-4a0c-8e17-e581a9a9300e",
   "metadata": {},
   "source": [
    "### Handling special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecc539-87c5-4fa7-9274-95f1981594cc",
   "metadata": {},
   "source": [
    "This code will remove all non-alphanumeric characters from each token in the 'tokens' column of your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a26d6-aa31-44fb-953a-9eb90cb78dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(lambda x: [re.sub(r'\\W', '', token) for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ea739-8026-4ce9-9191-e0cddc9cdc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3982bf9-d9af-411a-b2b3-57e39050fff9",
   "metadata": {},
   "source": [
    "### Removing blank rows   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b73af-426b-4acf-b881-a5978aa4be71",
   "metadata": {},
   "source": [
    "In the print-out above, I see cases (such as ID 39995) where the row is blank '[]'. These may add noise to the model and I will remove. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a7334-65c9-4304-8b3a-d2e2d0dcf0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the number of rows before removing blank rows\n",
    "num_rows_before = len(df)\n",
    "\n",
    "# Remove rows with blank lists in the 'tokens' column\n",
    "df = df[df['tokens'].apply(lambda tokens_list: tokens_list != [])]\n",
    "\n",
    "# Count the number of rows after removing blank rows\n",
    "num_rows_after = len(df)\n",
    "\n",
    "# Calculate the number of rows deleted\n",
    "num_rows_deleted = num_rows_before - num_rows_after\n",
    "\n",
    "# Print the number of rows deleted\n",
    "print(\"Number of rows deleted:\", num_rows_deleted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec776eb4-75c5-4d6f-bab9-594a477a4cf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Removing words with just one letter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1e535-a6c5-4a05-880d-de4e63d4708b",
   "metadata": {},
   "source": [
    "These are hidden stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34563721-34e7-49e6-aec8-4d8f476c0902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the target words to count\n",
    "target_words = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# Initialize counts for each target word\n",
    "word_counts = {word: 0 for word in target_words}\n",
    "\n",
    "# Iterate over the tokens and count occurrences of target words\n",
    "for tokens_list in df['tokens']:\n",
    "    for word in tokens_list:\n",
    "        for target_word in target_words:\n",
    "            if target_word == word:\n",
    "                word_counts[target_word] += 1\n",
    "            elif target_word.strip() == word:\n",
    "                word_counts[target_word] += 1\n",
    "\n",
    "# Print the word counts\n",
    "for word, count in word_counts.items():\n",
    "    print(f\"Occurrences of '{word}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43ea1d-b6e2-49f7-a7cc-0e3d441969f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete occurrences of target_words from df['tokens']\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens_list: [word for word in tokens_list if word not in target_words and word.strip() not in target_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5576a-f1f5-431f-814c-30d8d47dcbcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e44ac1-06cb-48d0-acdc-348441fe93bc",
   "metadata": {},
   "source": [
    "### Token normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6ee08-2111-40df-b7c4-de2ef78dca17",
   "metadata": {},
   "source": [
    "This involves techniques such as stemming or lemmatization to reduce words to their base or root forms. For example, \"running,\" \"runs,\" and \"ran\" may all be reduced to the base form \"run.\" This helps in capturing semantic similarities between related words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab52b5e-7f6b-48ba-b1f2-8f4ad6ed9a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to tokens\n",
    "df['tokens'] = df['tokens'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea64d0e-b34c-46cc-9997-528cd7c8add5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074274b-f2ca-49bf-879f-7d3f7e860af8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Handling out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89beb0c-ce0f-4d7a-802a-dc3df75cf3fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Author note: It seemed I couldn't do this until I had trained a Word2Vec model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f41aed-59a3-4e90-b9ab-f02f6240963d",
   "metadata": {},
   "source": [
    "## Reset index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6ed6f-8729-409c-a143-8ceffe969b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9eca9-a3c0-4e4f-8c9f-660cf3ef0008",
   "metadata": {},
   "source": [
    "## Check preprocessed df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b28960-d54a-444a-8ccc-1359caea126d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4195652-9fe7-4f84-9ed2-7aac349ca043",
   "metadata": {},
   "source": [
    "## Bag of words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2392ae7-e702-4f05-a836-a0898a12007d",
   "metadata": {},
   "source": [
    "Explanation: CountVectorizer is a text vectorization technique provided by scikit-learn (sklearn). It converts a collection of text documents into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec7074-20de-4e9f-83a4-fefe55fd5f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the text data and transform it into BoW representation\n",
    "bow_matrix = vectorizer.fit_transform(df['tokens'])\n",
    "\n",
    "# Convert BoW matrix to DataFrame for easier inspection\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the BoW DataFrame\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff94bc-3ed1-45bd-961c-183fc56baf03",
   "metadata": {},
   "source": [
    "## Concatenate bag of words matrix with original dataframe to enable modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59256a1-d9cf-4b26-8b90-c1dc027e6914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate bow_df with the original DataFrame df\n",
    "combined_df = pd.concat([df, bow_df], axis=1)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdde5a4-b10c-44d1-97a7-ed5692ee135f",
   "metadata": {},
   "source": [
    "## Balancing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e40d52-5233-48a2-a93f-184d61a7d9df",
   "metadata": {},
   "source": [
    "Author note: no data balancing applied in this model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e5cb18-4325-4738-8a60-a4c7b9441777",
   "metadata": {},
   "source": [
    "## Train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2d282-fbdd-4b0d-b396-c015789010fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = combined_df.drop(columns=['sentiment', 'content', 'sentiment_subgroup', 'tokens'])\n",
    "y = combined_df['sentiment_subgroup']\n",
    "\n",
    "# Perform train-test split with 80% training data and 20% testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the train and test sets\n",
    "print(\"Train set shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shapes:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6c921-91a1-4e55-a509-fb52ed7a8d43",
   "metadata": {},
   "source": [
    "Author note: Encountered major issues running code as of this point "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23779944-9bcf-49bd-b614-7160d3577a2a",
   "metadata": {},
   "source": [
    "## Train model using the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172c0f8-3083-4a24-9041-0677add89bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_ops = {\n",
    "    \"max_depth\": 6,\n",
    "    \"min_samples_leaf\": 20,\n",
    "    \"n_estimators\": 100,\n",
    "    \"bootstrap\": True,\n",
    "    \"oob_score\": True,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier(**rfc_ops)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"train prediction accuracy score: %.2f\" % (clf.score(X_train, y_train)))\n",
    "print(\"test prediction accuracy score: %.2f\" % (clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893023ab-8421-42ce-a84b-0383b8d20b76",
   "metadata": {},
   "source": [
    "- train prediction accuracy score 0.4\n",
    "- test prediction accuracy score 0.4\n",
    "\n",
    "The accuracy scores of 0.40 for both the training and testing sets suggest that the model is underfitting, which means it is too simple to capture the underlying structure of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
