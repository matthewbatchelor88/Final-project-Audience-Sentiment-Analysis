{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d58ce08-8fc4-4c43-9033-a9f61206ac26",
   "metadata": {},
   "source": [
    "## Cycle 3 Iteration 4 description "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03998420-c155-4629-aac6-a986e6c0444c",
   "metadata": {},
   "source": [
    "In this iteration, I build on Iteration 3 and create user interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f55890-f3f3-453c-9d59-b181f60a5e04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the file pathway:  /Users/matthewbatchelor/Downloads/test_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created successfully:\n",
      "   unique identifier                                               text\n",
      "0              72489  The service was exceptional! The staff was fri...\n",
      "1              53072  I had a terrible experience at your store. The...\n",
      "2              16947  I'm neutral about my experience at your establ...\n",
      "3              87532  The online ordering process was smooth and has...\n",
      "4              41293  I'm disappointed with the quality of the produ...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   unique identifier  20 non-null     int64 \n",
      " 1   text               20 non-null     object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 452.0+ bytes\n",
      "None\n",
      "- Preprocessing step 1/9: Null value check done. Number of dropped rows due to null values: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matthewbatchelor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matthewbatchelor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Preprocessing step 2/9: Text has been tokenized successfully!\n",
      "- Preprocessing step 3/9: Tokenized text lowercased successfully!\n",
      "- Preprocessing step 4/9: Punctuation removed from tokenized text successfully!\n",
      "- Preprocessing step 5/9: Stopwords removed from tokenized text successfully!\n",
      "- Preprocessing step 6/9: Blank text removed from tokenized text successfully!\n",
      "- Preprocessing step 7/9: Numerical values removed from tokenized text successfully!\n",
      "- Preprocessing step 8/9: Special characters removed from tokenized text successfully!\n",
      "- Preprocessing step 9/9: One letter words removed from tokenized text successfully!\n",
      "- Preprocessing complete!\n",
      "Tokenized and preprocessed text data has been stemmed with Porter Stemmer!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique identifier</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72489</td>\n",
       "      <td>The service was exceptional! The staff was fri...</td>\n",
       "      <td>servic except staff friendli help food delici ...</td>\n",
       "      <td>[-0.008174025, 0.025680337, 0.013656314, 0.022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53072</td>\n",
       "      <td>I had a terrible experience at your store. The...</td>\n",
       "      <td>terribl experi store staff rude product qualit...</td>\n",
       "      <td>[-0.010783804, 0.02686394, 0.01255794, 0.02368...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16947</td>\n",
       "      <td>I'm neutral about my experience at your establ...</td>\n",
       "      <td>neutral experi establish servic averag product...</td>\n",
       "      <td>[-0.01114888, 0.027538154, 0.013205601, 0.0243...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87532</td>\n",
       "      <td>The online ordering process was smooth and has...</td>\n",
       "      <td>onlin order process smooth hasslefre receiv pa...</td>\n",
       "      <td>[-0.01048251, 0.027087936, 0.014321848, 0.0252...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41293</td>\n",
       "      <td>I'm disappointed with the quality of the produ...</td>\n",
       "      <td>disappoint qualiti product receiv nt meet expect</td>\n",
       "      <td>[-0.0104486095, 0.025587244, 0.012627092, 0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique identifier                                               text  \\\n",
       "0              72489  The service was exceptional! The staff was fri...   \n",
       "1              53072  I had a terrible experience at your store. The...   \n",
       "2              16947  I'm neutral about my experience at your establ...   \n",
       "3              87532  The online ordering process was smooth and has...   \n",
       "4              41293  I'm disappointed with the quality of the produ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  servic except staff friendli help food delici ...   \n",
       "1  terribl experi store staff rude product qualit...   \n",
       "2  neutral experi establish servic averag product...   \n",
       "3  onlin order process smooth hasslefre receiv pa...   \n",
       "4   disappoint qualiti product receiv nt meet expect   \n",
       "\n",
       "                                     word_embeddings  \n",
       "0  [-0.008174025, 0.025680337, 0.013656314, 0.022...  \n",
       "1  [-0.010783804, 0.02686394, 0.01255794, 0.02368...  \n",
       "2  [-0.01114888, 0.027538154, 0.013205601, 0.0243...  \n",
       "3  [-0.01048251, 0.027087936, 0.014321848, 0.0252...  \n",
       "4  [-0.0104486095, 0.025587244, 0.012627092, 0.02...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# USER INPUT \n",
    "\n",
    "file_path = input(\"Please enter the file pathway: \")\n",
    "\n",
    "try:\n",
    "    # Attempt to read the file into a DataFrame\n",
    "    df = pd.read_csv(file_path)  # Assuming the file is a CSV file, change the function accordingly for other file types\n",
    "    print(\"DataFrame created successfully:\")\n",
    "    print(df.head())  # Print the first few rows of the DataFrame\n",
    "    print(df.info())  # Print info about the dataframe \n",
    "except FileNotFoundError:\n",
    "    # Handle the case when the file is not found\n",
    "    print(\"Error: File not found.\")\n",
    "except Exception as e:\n",
    "    # Handle any other exceptions that might occur\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "# NULL VALUES CHECK     \n",
    "\n",
    "# Check for null values \n",
    "\n",
    "# Count null values in 'review/text' column before dropping\n",
    "null_count_before = df['text'].isnull().sum()\n",
    "\n",
    "# Drop rows with null values in 'review/text' column\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# Count null values in 'review/text' column after dropping\n",
    "null_count_after = df['text'].isnull().sum()\n",
    "\n",
    "# Calculate the count of dropped rows\n",
    "dropped_count = null_count_before - null_count_after\n",
    "\n",
    "# Print the count of dropped rows\n",
    "print(\"- Preprocessing step 1/9: Null value check done. Number of dropped rows due to null values:\", dropped_count)\n",
    "\n",
    "# TOKENIZING\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure that you have the 'punkt' tokenizer models downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Convert values in the 'review/text' column to strings and then tokenize\n",
    "df['tokens'] = df['text'].astype(str).apply(word_tokenize)\n",
    "\n",
    "print(\"- Preprocessing step 2/9: Text has been tokenized successfully!\")\n",
    "\n",
    "# PREPROCESSING TOKENIZED DATA \n",
    "\n",
    "# Function to lowercase each word in a list of tokens\n",
    "def lowercase_tokens(tokens_list):\n",
    "    return [word.lower() for word in tokens_list]\n",
    "\n",
    "# Apply lowercase conversion to each list of tokens in the 'tokens' column\n",
    "df.loc[:, 'tokens'] = df['tokens'].apply(lowercase_tokens)\n",
    "print(\"- Preprocessing step 3/9: Tokenized text lowercased successfully!\")\n",
    "\n",
    "import re\n",
    "# Apply punctuation removal using a lambda function\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens_list: [re.sub(r'[^\\w\\s]', '', word) for word in tokens_list] if isinstance(tokens_list, list) else [])\n",
    "print(\"- Preprocessing step 4/9: Punctuation removed from tokenized text successfully!\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "# Get the English stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Remove stopwords from the 'tokens' column\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(\"- Preprocessing step 5/9: Stopwords removed from tokenized text successfully!\")\n",
    "\n",
    "# Remove blank text from the list of tokens\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens_list: [token for token in tokens_list if token.strip() != ''])\n",
    "print(\"- Preprocessing step 6/9: Blank text removed from tokenized text successfully!\")\n",
    "\n",
    "# Remove numerical values \n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token for token in x if not token.isdigit()])\n",
    "print(\"- Preprocessing step 7/9: Numerical values removed from tokenized text successfully!\")\n",
    "\n",
    "# Remove special characters \n",
    "df['tokens'] = df['tokens'].apply(lambda x: [re.sub(r'\\W', '', token) for token in x])\n",
    "print(\"- Preprocessing step 8/9: Special characters removed from tokenized text successfully!\")\n",
    "\n",
    "# Remove words with just one letter \n",
    "\n",
    "# Define the target words to count\n",
    "target_words = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# Initialize counts for each target word\n",
    "word_counts = {word: 0 for word in target_words}\n",
    "\n",
    "# Iterate over the tokens and count occurrences of target words\n",
    "for tokens_list in df['tokens']:\n",
    "    for word in tokens_list:\n",
    "        for target_word in target_words:\n",
    "            if target_word == word:\n",
    "                word_counts[target_word] += 1\n",
    "            elif target_word.strip() == word:\n",
    "                word_counts[target_word] += 1\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens_list: [word for word in tokens_list if word not in target_words and word.strip() not in target_words])\n",
    "print(\"- Preprocessing step 9/9: One letter words removed from tokenized text successfully!\")\n",
    "print(\"- Preprocessing complete!\")\n",
    "\n",
    "# TOKEN NORMALIZATION\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to tokens\n",
    "df['tokens'] = df['tokens'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x]))\n",
    "print(\"Tokenized and preprocessed text data has been stemmed with Porter Stemmer!\")\n",
    "\n",
    "# Reset index\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# WORD EMBEDDING\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained Word2Vec model \n",
    "word2vec_model = Word2Vec(df['tokens'], min_count=1)\n",
    "\n",
    "# Function to apply word embeddings\n",
    "def apply_word_embeddings(tokens):\n",
    "    word_vectors = []\n",
    "    for token in tokens:\n",
    "        if token in word2vec_model.wv:\n",
    "            word_vector = word2vec_model.wv[token]  # Get word vector\n",
    "            word_vectors.append(word_vector)\n",
    "    if word_vectors:\n",
    "        tweet_vector = np.mean(word_vectors, axis=0)  # Aggregate word vectors\n",
    "        return tweet_vector\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply word embeddings to the 'tokens' column\n",
    "df['word_embeddings'] = df['tokens'].apply(apply_word_embeddings)\n",
    "\n",
    "# Drop rows with None values in 'word_embeddings' column\n",
    "df = df.dropna(subset=['word_embeddings'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfcf201e-152e-4221-8169-067806d90a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test csv : /Users/matthewbatchelor/Downloads/test_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e5cb18-4325-4738-8a60-a4c7b9441777",
   "metadata": {},
   "source": [
    "## Make predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edcf430d-19fe-4a6e-a108-06e323d05b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 13:35:12.251446: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'create_model' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the saved model using pickle\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Convert word embeddings to numpy array\u001b[39;00m\n\u001b[1;32m     10\u001b[0m X_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'create_model' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "# Load the saved model using pickle\n",
    "with open('best_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Convert word embeddings to numpy array\n",
    "X_new = np.array(df['word_embeddings'].tolist())\n",
    "\n",
    "# Standardize features if necessary\n",
    "scaler = StandardScaler()\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = model.predict(X_new_scaled)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7c6e2-4081-4e26-8ad0-eb680865e0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
